{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 2: Data Wrangling, Exploratory Data Analysis (EDA), and Data Visualization**\n",
    "\n",
    "## **ðŸ“Œ Lecture 1: Data Wrangling & Exploratory Data Analysis (EDA)**\n",
    "\n",
    "\n",
    "### Recap of the Previous Session:\n",
    "\n",
    "1. Python Basics: Syntax and Data Types\n",
    "- Explored the core **data types** in Python: `int`, `float`, `str`, and `bool`.\n",
    "- Discussed **variables** and their role in storing data.\n",
    "- Introduced **control structures**, including `if-else` statements and loops (`for` and `while`).\n",
    "\n",
    "2. Data Structures in Python\n",
    "- Covered **Lists, Tuples, Dictionaries, and Sets** as fundamental structures for storing collections of data.\n",
    "\n",
    "3. Functions and Modules\n",
    "- Examined the importance of **functions** for code reusability and modularity.\n",
    "- Introduced the concept of **modules** and how they allow us to extend Pythonâ€™s capabilities.\n",
    "\n",
    "4. Object-Oriented Programming (OOP)\n",
    "- Learned about **classes** and how they help encapsulate logic and data.\n",
    "- Introduced the `FinancialCalculator` class, demonstrating methods for **simple and compound interest calculations**.\n",
    "- Expanded on OOP concepts with **inheritance**, applying it to economic models (e.g., `Consumer` classes).\n",
    "\n",
    "5. Coding Best Practices and PEP 8\n",
    "- Emphasized **code readability** and **consistent formatting** using the PEP 8 style guide.\n",
    "- Discussed **file and folder organization**, ensuring well-structured project directories.\n",
    "- Implemented meaningful **variable and function names**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Objectives for tonight:**\n",
    "- Introduction to **data frames** and how to work with structured data.  \n",
    "- Learn about **pandas DataFrames** and how they compare to Stata's `dataframe` and R's `data.frame`.\n",
    "- Explore **data wrangling techniques**, such as filtering, aggregating, and reshaping data to work with real-world datasets, applying methods to clean, transform, and analyze data.\n",
    "- Learn the essentials of **data wrangling techniques** for handling missing values, merging, and reshaping data.  \n",
    "- Learn how to conduct **Exploratory Data Analysis (EDA)** to understand key patterns in economic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: Understanding DataFrames in Python (~45 min)**\n",
    "\n",
    "### **1.1 Introduction to DataFrames** \n",
    "#### What is a DataFrame?\n",
    "- A **DataFrame** is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns).\n",
    "  - commonly used in data wrangling and exploratory data analysis.\n",
    "  - are part of the **pandas library**, a powerful data tool for Python.\n",
    "  - consists of three main components:\n",
    "    - *rows and columns* - similar to a spreadsheet or a dataset in Stata, or a `data.frame` in R.\n",
    "    - *labeled axes* - row labels (index) and column names.\n",
    "    - *heterogeneous data types* - each column can have a different data type (e.g., integer, float, string), unlike NumPy arrays.\n",
    "\n",
    "#### Python vs Stata vs R\n",
    "| Feature       | Python (pandas) | Stata | R (data.frame) |\n",
    "|--------------|----------------|--------|---------------|\n",
    "| Structure    | Two-dimensional, labeled table | Two-dimensional table | Two-dimensional table |\n",
    "| Indexing     | Explicit row/column labels | Implicit row numbers | Implicit row numbers |\n",
    "| Data Types   | Supports mixed types (int, float, object, categorical) | Primarily numeric and categorical | Supports mixed types |\n",
    "| Operations   | Vectorized operations (`.apply()`, `.groupby()`) | Built-in commands for regressions, summaries | Functional programming style |\n",
    "| Extensibility | Integrates with NumPy, SQL, ML libraries | Mostly standalone | Compatible with `dplyr`, `tidyverse` |\n",
    "| Performance  | Optimized for large datasets, fast operations | Good performance, but limited scalability | Efficient but depends on libraries (`data.table`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How DataFrames are Used in Econometric Analysis**\n",
    "In **econometrics**, DataFrames act as the core **data structure** for managing, manipulating, and analyzing datasets. They are widely used for:\n",
    "\n",
    "1. **Data Cleaning & Preparation**  \n",
    "   - Handling missing values, outliers, and data inconsistencies.\n",
    "   - Transforming data into a suitable format for analysis.\n",
    "   - Merging and reshaping datasets.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**  \n",
    "   - Summarizing data (`.describe()`, `.groupby()`)\n",
    "   - Checking **correlations** (`df.corr()`)\n",
    "   - Detecting **outliers** using **boxplots & histograms**\n",
    "\n",
    "3. **Regression Analysis**  \n",
    "   - Running **OLS regression** using `statsmodels`\n",
    "   - Handling **heteroskedasticity** and **multicollinearity**\n",
    "   - Performing **difference-in-differences (DiD)** analysis\n",
    "\n",
    "4. **Panel Data & Time Series**  \n",
    "   - **Merging** cross-sectional & time-series data (`merge()`)\n",
    "   - Running **fixed-effects & random-effects models**\n",
    "   - Handling **autocorrelation & stationarity tests**\n",
    "\n",
    "5. **Causal Inference & Policy Analysis**  \n",
    "   - Implementing **Instrumental Variable (IV) regressions**\n",
    "   - Propensity Score Matching (PSM) for treatment effects\n",
    "   - Monte Carlo simulations & bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titanic Dataset Example\n",
    "- To illustrate the concepts, we will use the **Titanic dataset**\n",
    "- The dataset contains information about passengers on the Titanic, including their survival\n",
    "- dimensions: 891 rows x 15 columns\n",
    "- features:\n",
    "    - `survived`: binary variable (0 = No, 1 = Yes)\n",
    "    - `pclass`: passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "    - `sex`: male or female\n",
    "    - `age`: age of the passenger (integer)\n",
    "    - `sibsp`: number of siblings/spouses aboard (integer)\n",
    "    - `parch`: number of parents/children aboard (integer)\n",
    "    - `fare`: ticket fare (float)\n",
    "    - `embarked`: port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "    - `class`: passenger class (1st, 2nd, 3rd)\n",
    "    - `who`: man, woman, or child\n",
    "    - `deck`: deck of the ship\n",
    "    - `embark_town`: town of embarkation\n",
    "    - `alive`: survival status (Yes or No)\n",
    "    - `alone`: whether the passenger was alone (True or False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Cleaning and Preparation\n",
    "\n",
    "#### Handling **missing values** \n",
    "- **Missing values** are common in real-world datasets and can affect the quality of analysis.\n",
    "- Common strategies for handling missing values include:\n",
    "  - **Dropping missing values**: Removing rows or columns with missing data. (`.dropna()`)\n",
    "  - **Imputing missing values**: Replacing missing values with a specific value (e.g., mean, median, mode). (`.fillna()`)\n",
    "  - **Forward-fill or backward-fill**: Propagating the last known value forward or backward. (`.ffill()`, `.bfill()`)\n",
    "  - **Interpolation**: Estimating missing values based on existing data points. (`.interpolate()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of missing values in each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another dataframe\n",
    "df_clean = df.copy()\n",
    "\n",
    "# drop rows in embarked with missing values\n",
    "df_clean = df_clean.dropna(subset=['embarked']) \n",
    "\n",
    "# drop deck column\n",
    "df_clean = df_clean.drop(columns=['deck'])\n",
    "\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values in age column\n",
    "df_clean['age_intfill'] = df_clean['age'].interpolate()\n",
    "print(df_clean['age'].mean(), df_clean['age'].median())\n",
    "print(df_clean['age_intfill'].mean(), df_clean['age_intfill'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating **dummy variables**\n",
    "- **Dummy variables** are binary (0 or 1) variables used to represent categorical data.\n",
    "- They are essential for including categorical variables in regression models.\n",
    "- In pandas, you can create dummy variables using the `pd.get_dummies()` function.\n",
    "- warning: \n",
    "    - **Dummy variable trap**: Avoid multicollinearity by dropping one of the dummy variables.\n",
    "    - **Dummy variable reference category**: Choose a reference category for interpretation.\n",
    "    - make sure to look at the contents of the columns to avoid multicollinearity or to make unnecessary columns that will not be used in the analysis since this increases the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for sex\n",
    "df_clean = pd.get_dummies(df_clean, columns=['sex'], drop_first=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming columns\n",
    "- **Column names** should be descriptive and follow a consistent naming convention.\n",
    "- You can rename columns using the `.rename()` method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename parch to parent_child\n",
    "df_clean.rename(columns={'parch': 'parent_child'}, inplace=True)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling duplicates\n",
    "- **Duplicate rows** can skew analysis results and should be removed.\n",
    "- To identify and remove duplicates, you can use the `.duplicated()` then `.drop_duplicates()` methods.\n",
    "- **Duplicated rows** are identified based on all column values being the same. \n",
    "- warning: \n",
    "    - **Dropping duplicates**: Be cautious when removing duplicates to avoid losing valuable data.\n",
    "    - **Identifying duplicates**: Check for duplicates based on specific columns or subsets of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(df_clean.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identigy the rows with duplicate values\n",
    "duplicates = df_clean[df_clean.duplicated(keep=False)].sort_values(by='age')\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(df_clean.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing data types\n",
    "- **Data types** in pandas can be changed to optimize memory usage or facilitate analysis.\n",
    "- Data types are changed to ensure consistency, accuracy, and efficiency in data operations.\n",
    "- examples:\n",
    "    - `pd.to_datetime()`: Convert a column to a datetime format.\n",
    "    - `astype()`: Convert a column to a specific data type (e.g., int, float, str).\n",
    "    - `pd.to_numeric()`: Convert a column to numeric data type.\n",
    "\n",
    "#### String Cleaning and Standardization\n",
    "- **String cleaning** involves removing leading/trailing spaces, converting to lowercase, and standardizing text.\n",
    "    - `.str.strip()`: Remove leading and trailing spaces.\n",
    "    - `.str.lower()`: Convert strings to lowercase.\n",
    "    - `.str.upper()`: Convert strings to uppercase.\n",
    "    - `.str.split()`: Split strings based on a delimiter.\n",
    "- Standardizing strings ensures consistency and accuracy in text-based analysis.\n",
    "    - `.str.contains()`: Check if a string contains a specific pattern. Returns a boolean mask.\n",
    "    - `.str.startswith()`: Check if a string starts with a specific pattern.\n",
    "    - `.str.replace()`: Replace a substring with another value. You can also use this to remove specific characters or patterns.\n",
    "\n",
    "\n",
    "#### Mapping and Replacing Values\n",
    "- **Mapping values** involves creating a dictionary to replace specific values in a column.\n",
    "- **Replacing values** allows you to substitute specific values with new values.\n",
    "- examples:\n",
    "    - `map()`: Map values based on a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of values to replace in embarked column\n",
    "embarked_dict = {'Southampton': 0, 'Cherbourg': 1, 'Queenstown': 2}\n",
    "\n",
    "# map embarked to numeric values\n",
    "df_clean['emb_dum'] = df_clean['embark_town'].map(embarked_dict)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting and Filtering Data\n",
    "- this is useful when you want to analyze a subset of the data.\n",
    "    - if you want to see the features of the passengers who survived\n",
    "    - in economics, you might want to see the features of the people who are employed or unemployed.\n",
    "- you can use the `.loc[]` and `.iloc[]` methods to select rows and columns based on labels or indices.\n",
    "    - `.loc[]`: Select rows and columns based on labels.\n",
    "    - `.iloc[]`: Select rows and columns based on indices.\n",
    "- when selecting a subset based on specific conditions:\n",
    "    - `df[df['column_name'] > value]`: Select rows based on a condition.\n",
    "    - `df[(df['column_name'] > value) & (df['column_name'] < value)]`: Select rows based on multiple conditions.\n",
    "- disadvantage of stata: it is difficult to select and filter data in stata compared to python and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select all the survivors\n",
    "df_clean_survived = df_clean[df_clean['survived'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging & Reshaping Data**\n",
    "#### Merging DataFrames\n",
    "- **Merging** combines two or more DataFrames based on common columns or indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/moxballo/Documents/GitHub/ds4upse-2425s2/\"\n",
    "apis_path = path + \"03_data/PHL-PSA-APIS-2022-PUF/\"\n",
    "\n",
    "hh_rec = pd.read_csv(apis_path + \"APIS PUF 2022 Household Record.CSV\")\n",
    "mem_rec = pd.read_csv(apis_path + \"APIS PUF 2022 Member Record.CSV\")\n",
    "apis_rtg1 = pd.read_csv(apis_path + \"APIS PUF 2022 RTG1 - Social Protection - Social Insurance.CSV\")\n",
    "apis_rtg2 = pd.read_csv(apis_path + \"APIS PUF 2022 RTG2 - Social Protection - Social Assistance.CSV\")\n",
    "apis_rtg3 = pd.read_csv(apis_path + \"APIS PUF 2022 RTG3 - Social Protection - Labor Market Intervention.CSV\")\n",
    "apis_rtg4 = pd.read_csv(apis_path + \"APIS PUF 2022 RTG4 - Government Feeding Program.CSV\")\n",
    "apis_rtg5 = pd.read_csv(apis_path + \"APIS PUF 2022 RTG5 - Social Protection - Philhealth, Children, Disaster Preparedness and Recovery.CSV\")\n",
    "apis_rth = pd.read_csv(apis_path + \"APIS PUF 2022 RTH - Access to Government Services.CSV\")\n",
    "apis_rti = pd.read_csv(apis_path + \"APIS PUF 2022 RTI - Housing.CSV\")\n",
    "apis_rtj = pd.read_csv(apis_path + \"APIS PUF 2022 RTJ - Water Sanitation and Hygiene.CSV\")\n",
    "apis_rtk = pd.read_csv(apis_path + \"APIS PUF 2022 RTK - Other Relevant Information.CSV\", on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the .info of the dataframes\n",
    "apis = {\n",
    "    \"Household Record\": hh_rec,\n",
    "    \"Member Record\": mem_rec,\n",
    "    \"RTG1 - Social Insurance\": apis_rtg1,\n",
    "    \"RTG2 - Social Assistance\": apis_rtg2,\n",
    "    \"RTG3 - Labor Market Intervention\": apis_rtg3,\n",
    "    \"RTG4 - Government Feeding Program\": apis_rtg4,\n",
    "    \"RTG5 - Philhealth, Children, Disaster Prep and Recovery\": apis_rtg5,\n",
    "    \"RTH - Access to Government Services\": apis_rth,\n",
    "    \"RTI - Housing\": apis_rti,\n",
    "    \"RTJ - Water Sanitation and Hygiene\": apis_rtj,\n",
    "    \"RTK - Other Relevant Information\": apis_rtk\n",
    "}\n",
    "\n",
    "# loop through the dictionary and print the .info of each dataframe\n",
    "for name, data in apis.items():\n",
    "    print(f\"Dataset: {name}\" )\n",
    "    print(\"shape:\", data.shape)\n",
    "    print(\"column data types:\", data.dtypes)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique number of \"HHID\" for each dataframe\n",
    "for name, data in apis.items():\n",
    "    print(f\"Dataset: {name}\" )\n",
    "    print(\"unique HHID:\", data['HHID'].nunique())\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce each dataframe to the first instance of each unique \"HHID\"\n",
    "for name, data in apis.items():\n",
    "    data.drop_duplicates(subset='HHID', keep='first', inplace=True)\n",
    "    print(f\"Dataset: {name}\" )\n",
    "    print(\"shape:\", data.shape)\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the dataframes into one dataframe using the HHID as the key\n",
    "apis_4merge = {\n",
    "    \"Member Record\": mem_rec,\n",
    "    \"RTG1 - Social Insurance\": apis_rtg1,\n",
    "    \"RTG2 - Social Assistance\": apis_rtg2,\n",
    "    \"RTG3 - Labor Market Intervention\": apis_rtg3,\n",
    "    \"RTG4 - Government Feeding Program\": apis_rtg4,\n",
    "    \"RTG5 - Philhealth, Children, Disaster Prep and Recovery\": apis_rtg5,\n",
    "    \"RTH - Access to Government Services\": apis_rth,\n",
    "    \"RTI - Housing\": apis_rti,\n",
    "    \"RTJ - Water Sanitation and Hygiene\": apis_rtj,\n",
    "    \"RTK - Other Relevant Information\": apis_rtk\n",
    "}\n",
    "\n",
    "apis_merged = hh_rec.copy()\n",
    "\n",
    "for name, data in apis_4merge.items():\n",
    "    data_copy = data.copy()\n",
    "    data_copy.drop(columns=['REG'], inplace=True)\n",
    "    apis_merged = apis_merged.merge(data_copy, on='HHID', how='left')\n",
    "\n",
    "print(apis_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_merged['K5'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_merged_clean = apis_merged.dropna(subset=['K5'])\n",
    "print(apis_merged_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Merging\n",
    "- we have demonstrated 'left' merging which is the most common type of merging.\n",
    "- other types of merging include:\n",
    "    - **Inner join**: Keep only the rows with matching keys in both DataFrames.\n",
    "    - **Outer join**: Keep all rows from both DataFrames, filling in missing values with NaN.\n",
    "    - **Right join**: Keep all rows from the right DataFrame, filling in missing values with NaN.\n",
    "    - **Left join**: Keep all rows from the left DataFrame, filling in missing values with NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Engineering**\n",
    "- **Feature engineering** involves creating new features from existing data, usually to improve model performance in data science\n",
    "- Economics applications include creating interaction terms, polynomial features, and lagged variables.\n",
    "- examples:\n",
    "    - generating per capita variables\n",
    "    - Creating interaction terms: Multiply two or more variables to capture combined effects.\n",
    "    - Polynomial features: Generate polynomial terms (e.g., squared, cubed) to capture non-linear relationships.\n",
    "    - Lagged variables: Create lagged variables to account for time dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_merged_clean['FSIZE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_merged_clean['hh_income_pc'] = apis_merged_clean['K5'] / apis_merged_clean['FSIZE']\n",
    "apis_merged_clean['hh_income_pc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to EDA**\n",
    "\n",
    "### **Summary Statistics**\n",
    "- **Summary statistics** provide a quick overview of the data distribution.\n",
    "- Common summary statistics include:\n",
    "    - **Count**: Number of non-missing values in a dataset.\n",
    "    - **Mean**: Average value of a variable.\n",
    "    - **Median**: Middle value of a variable.\n",
    "    - **Standard deviation**: Measure of data dispersion.\n",
    "    - **Minimum and maximum**: Smallest and largest values in a dataset.\n",
    "    - **Quantiles**: Values that divide the data into equal parts (e.g., quartiles, deciles).\n",
    "- Easiest way to generate summary statistics is by using the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics of 'K5' and 'ave_hh_inc' side by side\n",
    "print(apis_merged_clean[['K5', 'hh_income_pc']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis\n",
    "- **Univariate analysis** focuses on analyzing a single variable in isolation.\n",
    "- Common univariate analysis techniques include:\n",
    "    - **Histograms**: Visualize the distribution of a variable.\n",
    "    - **Box plots**: Display the distribution of a variable and identify outliers.\n",
    "    - **Bar plots**: Show the frequency of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a distribution of 'hh_income_pc'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(apis_merged_clean['hh_income_pc'], bins=10, kde=True)\n",
    "plt.xlabel(\"Household Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Household Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of 'hh_income_pc' and 'K5'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=apis_merged_clean[['K5', 'hh_income_pc']])\n",
    "plt.ylabel(\"Household Income\")\n",
    "plt.title(\"Boxplot of Household Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the outliers 95\n",
    "outliers = apis_merged_clean[apis_merged_clean['hh_income_pc'] > apis_merged_clean['hh_income_pc'].quantile(0.99)]\n",
    "print(outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'hh_income_pc' without the outliers\n",
    "\n",
    "apis_no_outliers = apis_merged_clean[apis_merged_clean['hh_income_pc'] <= apis_merged_clean['hh_income_pc'].quantile(0.99)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(apis_no_outliers['hh_income_pc'], bins=10, kde=True)\n",
    "plt.xlabel(\"Household Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Household Income\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
